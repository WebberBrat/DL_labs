{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8406b6b2-5c38-42e1-bdd2-c0ebf5fc0307",
   "metadata": {},
   "source": [
    "## PyTorch exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466028e0-871c-4c72-868b-0b8439e659be",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "1. Make a tensor of size (2, 17)\n",
    "2. Make a torch.FloatTensor of size (3, 1)\n",
    "3. Make a torch.LongTensor of size (5, 2, 1)\n",
    "  - fill the entire tensor with 7s\n",
    "4. Make a torch.ByteTensor of size (5,)\n",
    "  - fill the middle 3 indices with ones such that it records [0, 1, 1, 1, 0]\n",
    "5. Perform a matrix multiplication of two tensors of size (2, 4) and (4, 2). Then do it in-place.\n",
    "6. Do element-wise multiplication of two randomly filled $(n_1,n_2,n_3)$ tensors. Then store the result in an Numpy array.\n",
    "\n",
    "### Forward-prop/backward-prop\n",
    "1. Create a Tensor that `requires_grad` of size (5, 5).\n",
    "2. Sum the values in the Tensor.\n",
    "3. Multiply the tensor by 2 and assign the result to a new python variable (i.e. `x = result`)\n",
    "4. Sum the variable's elements and assign to a new python variable\n",
    "5. Print the gradients of all the variables\n",
    "6. Now perform a backward pass on the last variable (NOTE: for each new python variable that you define, call `.retain_grad()`)\n",
    "7. Print all gradients again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d7d57-678c-4bf1-8754-06d1383e9884",
   "metadata": {},
   "source": [
    "### Deep-forward NNs\n",
    "1. Look at Lab 3. In Exercise 12 there, you had to build an $L$-layer neural network with the following structure: *[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*. Reimplement the manual code in PyTorch.\n",
    "2. Compare test accuracy using different optimizers: SGD, Adam, Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5366761b-c026-4fba-a64e-07c57fef0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3eb55e5c-4bb8-460e-9b84-ce0b294e868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape: torch.Size([2, 17])\n",
      "B.dtype, B.shape: torch.float32 torch.Size([3, 1])\n",
      "C.dtype, C.shape: torch.int64 torch.Size([5, 2, 1])\n",
      "=== 4. ByteTensor ===\n",
      "ByteTensor: tensor([0, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "\n",
      "=== 5. Matrix Multiplication ===\n",
      "Result (normal):\n",
      " tensor([[-1.5341, -2.1713],\n",
      "        [-0.7821, -0.4532]])\n",
      "Result (in-place with out=):\n",
      " tensor([[-1.5341, -2.1713],\n",
      "        [-0.7821, -0.4532]])\n",
      "\n",
      "=== 6. Element-wise Mul & NumPy Conversion ===\n",
      "t3 shape: torch.Size([3, 4, 5]) | NumPy array shape: (3, 4, 5)\n",
      "NumPy array dtype: float32\n",
      "\n",
      "=== Forward/Backward Propagation ===\n",
      "Gradients before backward:\n",
      "x0.grad: None\n",
      "s1.grad: None\n",
      "x.grad: None\n",
      "s2.grad: None\n",
      "\n",
      "Gradients after backward:\n",
      "x0.grad: tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]])\n",
      "s1.grad: None\n",
      "x.grad: tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "s2.grad: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 1. Tensor of size (2, 17)\n",
    "A = torch.rand(2, 17)\n",
    "print(\"A.shape:\", A.shape)\n",
    "\n",
    "# 2. FloatTensor of size (3, 1)\n",
    "B = torch.FloatTensor(3, 1).uniform_(0, 1)\n",
    "print(\"B.dtype, B.shape:\", B.dtype, B.shape)\n",
    "\n",
    "# 3. LongTensor of size (5, 2, 1)\n",
    "C = torch.LongTensor(5, 2, 1).fill_(7)\n",
    "print(\"C.dtype, C.shape:\", C.dtype, C.shape)\n",
    "\n",
    "print(\"=== 4. ByteTensor ===\")\n",
    "# 4. ByteTensor of size (5,), filled [0,1,1,1,0]\n",
    "byte_tensor = torch.zeros(5, dtype=torch.uint8)\n",
    "byte_tensor[1:4] = 1\n",
    "print(\"ByteTensor:\", byte_tensor)\n",
    "\n",
    "print(\"\\n=== 5. Matrix Multiplication ===\")\n",
    "# 5. Matrix multiplication (2,4) @ (4,2)\n",
    "mat1 = torch.randn(2, 4)\n",
    "mat2 = torch.randn(4, 2)\n",
    "# Normal matmul\n",
    "res = torch.mm(mat1, mat2)\n",
    "print(\"Result (normal):\\n\", res)\n",
    "\n",
    "# In-place using out= pre-allocated tensor\n",
    "res_inplace = torch.empty(2, 2)\n",
    "torch.mm(mat1, mat2, out=res_inplace)\n",
    "print(\"Result (in-place with out=):\\n\", res_inplace)\n",
    "\n",
    "print(\"\\n=== 6. Element-wise Mul & NumPy Conversion ===\")\n",
    "# 6. Element-wise multiplication of (n1,n2,n3) tensors\n",
    "n1, n2, n3 = 3, 4, 5\n",
    "t1 = torch.rand(n1, n2, n3)\n",
    "t2 = torch.rand(n1, n2, n3)\n",
    "t3 = t1 * t2\n",
    "np_array = t3.numpy()\n",
    "print(\"t3 shape:\", t3.shape, \"| NumPy array shape:\", np_array.shape)\n",
    "print(\"NumPy array dtype:\", np_array.dtype)\n",
    "\n",
    "print(\"\\n=== Forward/Backward Propagation ===\")\n",
    "# 1. Create a Tensor that requires grad\n",
    "x0 = torch.randn(5, 5, requires_grad=True)\n",
    "\n",
    "# 2. Sum the values\n",
    "s1 = x0.sum()\n",
    "s1.retain_grad()\n",
    "\n",
    "# 3. Multiply the tensor by 2\n",
    "x = x0 * 2\n",
    "x.retain_grad()\n",
    "\n",
    "# 4. Sum the new variable\n",
    "s2 = x.sum()\n",
    "s2.retain_grad()\n",
    "\n",
    "# 5. Print gradients before backward\n",
    "print(\"Gradients before backward:\")\n",
    "print(\"x0.grad:\", x0.grad)\n",
    "print(\"s1.grad:\", s1.grad)\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"s2.grad:\", s2.grad)\n",
    "\n",
    "# 6. Backward on last variable\n",
    "s2.backward()\n",
    "\n",
    "# 7. Print gradients after backward\n",
    "print(\"\\nGradients after backward:\")\n",
    "print(\"x0.grad:\", x0.grad)\n",
    "print(\"s1.grad:\", s1.grad)\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"s2.grad:\", s2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cab5c95-049d-4b73-8f85-aaf82bc309d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLayerNet(nn.Module):\n",
    "    def __init__(self, layer_dims):\n",
    "        \"\"\"\n",
    "        layer_dims: list of sizes, e.g. [784, 128, 64, 10]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for in_dim, out_dim in zip(layer_dims[:-1], layer_dims[1:]):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            layers.append(nn.Sigmoid())\n",
    "        layers.pop()  # remove last Sigmoid to allow logits for CrossEntropyLoss\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten if needed\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b89e89a6-4508-45dc-9d39-f3cff19b256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "train_ds = datasets.MNIST(root=\"mnist_data\", train=True,  download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"mnist_data\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e24b3cf2-51f1-469b-abe1-5f3fe9e54698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X).argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "    return correct / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b80c7ec-a25a-4f05-a972-780cc9b3e578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD        test accuracy: 80.64%\n",
      "Adam       test accuracy: 94.84%\n",
      "Momentum   test accuracy: 94.87%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dims = [28*28, 128, 64, 10]\n",
    "results = {}\n",
    "\n",
    "for opt_name, opt_fn in [\n",
    "    (\"SGD\", lambda p: torch.optim.SGD(p, lr=0.1)),\n",
    "    (\"Adam\", lambda p: torch.optim.Adam(p, lr=0.001)),\n",
    "    (\"Momentum\", lambda p: torch.optim.SGD(p, lr=0.1, momentum=0.9))\n",
    "]:\n",
    "    model = LLayerNet(dims).to(device)\n",
    "    optimizer = opt_fn(model.parameters())\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # train for 3 epochs (demo)\n",
    "    for epoch in range(3):\n",
    "        train_loss = train_one_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    test_acc = evaluate(model, test_loader, device)\n",
    "    results[opt_name] = test_acc\n",
    "    print(f\"{opt_name:10s} test accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89f409-ed32-421b-a1bd-9f2311b9822d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
